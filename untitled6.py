# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HLAQx0oih8gbnwBaPxQqPMkmv3H3Sm6o
"""

import os
import numpy as np
import matplotlib.pyplot as plt
import cv2
import shutil
from pathlib import Path
import random
import zipfile
from zipfile import ZipFile
import tensorflow as tf
from tensorflow.keras.backend import clear_session
from tensorflow.keras import layers
from tensorflow.keras import regularizers
from tensorflow.keras.preprocessing.image import img_to_array, array_to_img
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Conv2DTranspose, Reshape, LeakyReLU, Input
from tensorflow.keras.applications import InceptionV3
from tensorflow.keras.layers import GlobalAveragePooling2D
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from tensorflow.keras.optimizers.schedules import ExponentialDecay
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.regularizers import l2
from tensorflow.keras.models import load_model
from PIL import Image
import time


# Define constants
train_dir = '/content/data/Training'
output_dir = '/content/generated_images'
image_size = 256
batch_size = 16
epochs = 2

# Create a label mapping
label_names = sorted([d for d in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, d))])
label_to_index = {name: index for index, name in enumerate(label_names)}
num_classes = len(label_names)  # Number of classes

# Create a lookup table
keys_tensor = tf.constant(list(label_to_index.keys()))
vals_tensor = tf.constant(list(label_to_index.values()))
table = tf.lookup.StaticHashTable(
    tf.lookup.KeyValueTensorInitializer(keys_tensor, vals_tensor), default_value=-1)

# Function to load and preprocess images
def load_and_preprocess_image(file_path):
    img = tf.io.read_file(file_path)
    img = tf.image.decode_jpeg(img, channels=3)
    img = tf.image.resize(img, (image_size, image_size))
    img = img / 255.0  # Normalize to [0,1] range
    return img

# Function to get label from file path
def get_label(file_path):
    parts = tf.strings.split(file_path, os.path.sep)
    label = parts[-2]
    return table.lookup(label)

# Function to load image and label
def load_image_and_label(file_path):
    label = get_label(file_path)
    image = load_and_preprocess_image(file_path)
    return image, label

# Load and preprocess images from directory
train_dataset = tf.data.Dataset.list_files(train_dir + '/*/*')
train_dataset = train_dataset.map(load_image_and_label)
train_dataset = train_dataset.shuffle(buffer_size=1000).batch(batch_size)

# Define generator model
def make_generator_model():
    model = tf.keras.Sequential()
    model.add(layers.Dense(64*64*256, use_bias=False, input_shape=(100 + num_classes,)))
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Reshape((64, 64, 256)))
    assert model.output_shape == (None, 64, 64, 256)  # Note: None is the batch size

    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))
    assert model.output_shape == (None, 64, 64, 128)
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))
    assert model.output_shape == (None, 128, 128, 64)
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))
    assert model.output_shape == (None, image_size, image_size, 3)

    return model

# Define discriminator model
def make_discriminator_model():
    model = tf.keras.Sequential()
    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[image_size, image_size, 3 + num_classes]))
    model.add(layers.LeakyReLU())
    model.add(layers.Dropout(0.3))

    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))
    model.add(layers.LeakyReLU())
    model.add(layers.Dropout(0.3))

    model.add(layers.Flatten())
    model.add(layers.Dense(1))

    return model

# Define loss functions
cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)

# Discriminator loss
def discriminator_loss(real_output, fake_output):
    real_loss = cross_entropy(tf.ones_like(real_output), real_output)
    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)
    total_loss = real_loss + fake_loss
    return total_loss

# Generator loss
def generator_loss(fake_output):
    return cross_entropy(tf.ones_like(fake_output), fake_output)

# Optimizers
generator_optimizer = tf.keras.optimizers.Adam(1e-4)
discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)

# Define the models
generator = make_generator_model()
discriminator = make_discriminator_model()

# Define checkpoints
checkpoint_dir = './training_checkpoints'
checkpoint_prefix = os.path.join(checkpoint_dir, "ckpt")
checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,
                                 discriminator_optimizer=discriminator_optimizer,
                                 generator=generator,
                                 discriminator=discriminator)

# Define training step function
@tf.function
def train_step(images, labels):
    noise = tf.random.normal([batch_size, 100])
    noise = tf.concat([noise, tf.one_hot(labels, num_classes)], axis=1)

    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
        generated_images = generator(noise, training=True)

        real_labels = tf.one_hot(labels, num_classes)
        real_labels = tf.reshape(real_labels, [batch_size, 1, 1, num_classes])
        real_labels = tf.tile(real_labels, [1, image_size, image_size, 1])

        fake_labels = tf.one_hot(labels, num_classes)
        fake_labels = tf.reshape(fake_labels, [batch_size, 1, 1, num_classes])
        fake_labels = tf.tile(fake_labels, [1, image_size, image_size, 1])

        real_input = tf.concat([images, real_labels], axis=-1)
        fake_input = tf.concat([generated_images, fake_labels], axis=-1)

        real_output = discriminator(real_input, training=True)
        fake_output = discriminator(fake_input, training=True)

        gen_loss = generator_loss(fake_output)
        disc_loss = discriminator_loss(real_output, fake_output)

    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)
    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)

    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))
    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))

    return gen_loss, disc_loss

# Define function to generate and save images
def generate_and_save_images(model, epoch, test_input, labels):
    predictions = model(test_input, training=False)

    # Create output directory if not exists
    os.makedirs(output_dir, exist_ok=True)

    for i in range(predictions.shape[0]):
        class_folder = os.path.join(output_dir, f'class_{labels[i].numpy()}')
        os.makedirs(class_folder, exist_ok=True)
        cv2.imwrite(os.path.join(class_folder, f'image_{epoch}_{i}.jpg'), (predictions[i].numpy() * 127.5 + 127.5).astype(np.uint8))

# Define main training loop
def train(dataset, epochs):
    for epoch in range(epochs):
        start = time.time()
        for image_batch, label_batch in dataset:
            gen_loss, disc_loss = train_step(image_batch, label_batch)
        
        print(f'Epoch {epoch+1}, gen loss={gen_loss}, disc loss={disc_loss}')

        if (epoch + 1) % 50 == 0:
            noise = tf.random.normal([num_classes, 100])
            noise = tf.concat([noise, tf.one_hot(tf.range(num_classes), num_classes)], axis=1)
            generate_and_save_images(generator, epoch + 1, noise, tf.range(num_classes))

        # Save the model every 15 epochs
        if (epoch + 1) % 15 == 0:
            checkpoint.save(file_prefix=checkpoint_prefix)
        
        print(f'Time for epoch {epoch+1} is {time.time()-start} sec')

# Define seed for image generation
num_classes = 16  # Update this according to dataset
seed = tf.random.normal([num_classes, 100])
seed = tf.concat([seed, tf.one_hot(tf.range(num_classes), num_classes)], axis=1)


train(train_dataset, epochs)

